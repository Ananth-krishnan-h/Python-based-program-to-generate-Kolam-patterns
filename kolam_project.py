## -*- coding: utf-8 -*-
"""Kolam_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t9pY1ZrsQnKzrp2vr74fwkTEpNODdoH4

#Style Transfer Engine
"""

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.applications import VGG19
from tensorflow.keras import models, layers
import os
import cv2

"""## Define VGG19 content and style layers"""

CONTENT_LAYERS = ['block5_conv2']
STYLE_LAYERS = [
    'block1_conv1', 'block2_conv1', 'block3_conv1',
    'block4_conv1', 'block5_conv1'
]

"""## --- Image Loading and Preprocessing ---"""

def load_and_preprocess_image(path_to_img, size=(224, 224)):
    img = tf.io.read_file(path_to_img)
    img = tf.image.decode_image(img, channels=3)
    img = tf.image.convert_image_dtype(img, tf.float32)
    img = tf.image.resize(img, size)
    img = img[tf.newaxis, :]
    return img

def generate_dot_grid(size=(224, 224), dot_spacing=20):
    grid = np.zeros(size, dtype=np.uint8)
    for y in range(0, size[1], dot_spacing):
        for x in range(0, size[0], dot_spacing):
            grid[y, x] = 255
    grid = np.stack([grid, grid, grid], axis=-1)
    grid = tf.convert_to_tensor(grid, dtype=tf.float32)
    grid = grid / 255.0
    return grid[tf.newaxis, :]

"""## --- VGG19 Model and Loss Calculation ---"""

def get_vgg_model():
    vgg = VGG19(weights='imagenet', include_top=False)
    vgg.trainable = False

    style_outputs = [vgg.get_layer(name).output for name in STYLE_LAYERS]
    content_outputs = [vgg.get_layer(name).output for name in CONTENT_LAYERS]
    model_outputs = style_outputs + content_outputs

    return models.Model(inputs=vgg.input, outputs=model_outputs)

def get_style_loss(base_style, generated_style):
    style_loss = 0
    for base_feat, gen_feat in zip(base_style, generated_style):
        gram_base = gram_matrix(base_feat)
        gram_gen = gram_matrix(gen_feat)
        style_loss += tf.reduce_mean(tf.square(gram_base - gram_gen))
    return style_loss

def get_content_loss(base_content, generated_content):
    return tf.reduce_mean(tf.square(base_content - generated_content))

def gram_matrix(input_tensor):
    channels = int(input_tensor.shape[-1])
    a = tf.reshape(input_tensor, [-1, channels])
    n = tf.shape(a)[0]
    gram = tf.matmul(a, a, transpose_a=True)
    return gram / tf.cast(n, tf.float32)

"""#Reward Model (for User Feedback)"""

def build_reward_model(input_shape=(224, 224, 3)):
    """A simple CNN to act as the reward model."""
    model = models.Sequential([
        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),
        layers.MaxPooling2D((2, 2)),
        layers.Conv2D(64, (3, 3), activation='relu'),
        layers.MaxPooling2D((2, 2)),
        layers.Flatten(),
        layers.Dense(64, activation='relu'),
        layers.Dense(1)  # Output a single reward score
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

"""#The Feedback Loop(conceptual)"""

# --- SIMULATED USER FEEDBACK ---
# In a real-world scenario, you would have a dataset of images and their ratings.
# For this code, we will assume we have a simple function to get feedback.
def get_user_feedback(image_array):
    # This is a placeholder. You would replace this with an actual user interface.
    # For now, we will assign a random reward score.
    return np.random.uniform(0.1, 1.0) # Simulate a score between 0.1 and 1.0

# --- Training the Reward Model ---
def train_reward_model(images, ratings, reward_model):
    """
    Trains the reward model on a dataset of images and their human ratings.
    Images should be a numpy array of shape (num_samples, 224, 224, 3).
    Ratings should be a numpy array of shape (num_samples, 1).
    """
    reward_model.fit(images, ratings, epochs=10, batch_size=32)
    print("Reward model trained successfully.")

    # You would save this model for future use
    reward_model.save('reward_model.h5')

"""#The Combined Training Loop"""

def train_with_feedback(content_path, style_path, reward_model, epochs=1000, style_weight=1e-4, reward_weight=1e-2):
    # Prepare the images
    content_image = generate_dot_grid()
    style_image = load_and_preprocess_image(style_path)

    # Initialize the image to be generated
    generated_image = tf.Variable(content_image)

    # Get VGG feature extractor
    extractor = get_vgg_model()

    # Get initial feature maps for content and style
    all_features_base = extractor(style_image) # Use the kolam image as the base for style
    style_features_base = all_features_base[0:len(STYLE_LAYERS)]

    # Optimizer for the generated image
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.02, epsilon=1e-1)

    # Begin training
    print("Beginning combined training...")
    for i in range(epochs):
        with tf.GradientTape() as tape:
            # Get VGG features of the generated image
            all_features_gen = extractor(generated_image)
            style_features_gen = all_features_gen[0:len(STYLE_LAYERS)]

            # Calculate style loss
            style_loss = get_style_loss(style_features_base, style_features_gen)
            
            # --- The Content Loss has been REMOVED ---
            # It was causing the model to get stuck on the dot grid.
            # The model will now rely on style and reward feedback to create the kolam.
            
            # Calculate the reward loss
            reward_score = reward_model(generated_image)
            reward_loss = -1 * reward_score # Minimize negative reward to maximize it

            # Combine all losses
            total_loss = style_weight * style_loss + reward_weight * reward_loss

        # Optimize the generated image based on the combined loss
        grads = tape.gradient(total_loss, generated_image)
        optimizer.apply_gradients([(grads, generated_image)])

        # Print progress
        if (i+1) % 100 == 0:
            # Corrected line to fix the TypeError
            print(f"Epoch {i+1}: Total Loss={total_loss.numpy().item():.2f}, Reward Score={reward_score[0][0].numpy().item():.2f}")

    return generated_image

if __name__ == '__main__':
    # --- Part 1: Initial Generation without Feedback (Optional but Recommended) ---
    print("--- Phase 1: Generating a Kolam with pure Style Transfer ---")

    # You will need to replace 'your_kolam.jpg' with a path to one of your curated images
    # Make sure this image is in the same folder as the script
    style_image_path = 'kolam19-0.jpg'

    # Create the reward model
    reward_model = build_reward_model()

    # --- Part 2: Simulate User Feedback and Train the Reward Model ---
    # In a real pipeline, this would be an ongoing process
    print("--- Phase 2: Training the Reward Model ---")

    # Simulate a small dataset of 10 images with random ratings
    num_samples = 10
    simulated_images = [generate_dot_grid().numpy()[0] for _ in range(num_samples)]
    simulated_images = np.array(simulated_images)
    simulated_ratings = np.array([get_user_feedback(img) for img in simulated_images])

    # Train the reward model on this simulated data
    train_reward_model(simulated_images, simulated_ratings, reward_model)

    # --- Part 3: The Combined, Feedback-Driven Training ---
    print("--- Phase 3: Generating a Kolam with Human-Feedback-Driven Model ---")
    generated_kolam = train_with_feedback('placeholder', style_image_path, reward_model, epochs=2000)

    # Save the final image
    generated_kolam_final = tf.squeeze(generated_kolam, axis=0).numpy()
    plt.imshow(generated_kolam_final)
    plt.axis('off')
    plt.title('Final Generated Kolam')
    plt.savefig('final_generated_kolam.png')
    plt.show()


    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--inference', action='store_true',
                        help='Run generation-only (no training).')
    args = parser.parse_args()

    # Replace this with your actual generation function
    # Example: generate_and_save(output_path='final_generated_kolam.png')
    if args.inference:
        # call the fast inference/generation entrypoint
        generate_and_save('final_generated_kolam.png')
    else:
        # full script behaviour
        main()   # or whatever your current main() is
